{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import functools\n",
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "from utils import make_alibias, basis_emb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = torch.nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = torch.nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.register_buffer(\"alibias\", make_alibias(config.block_size, ms=torch.arange(1,config.n_head+1)*.05))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att += self.alibias[:, :, :T, :T]\n",
    "\n",
    "\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = torch.nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = torch.nn.GELU()\n",
    "        self.c_proj  = torch.nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wte = torch.nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            drop = torch.nn.Dropout(config.dropout),\n",
    "            h = torch.nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "        # self.head = torch.nn.ModuleList([torch.nn.Linear(config.n_embd, config.vocab_size, bias=False) for _ in range(config.output_dim)])\n",
    "            \n",
    "\n",
    "        self.head = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "          torch.nn.Linear(config.n_embd, config.vocab_size, bias=True)\n",
    "        ) for _ in range(config.output_dim)])\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.amax(dim=1)\n",
    "        out = []\n",
    "        for head in self.head:\n",
    "            logits = head(x)\n",
    "            out.append(logits)\n",
    "        out = torch.stack(out, dim=1)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    output_dim: int = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_up_target(X):\n",
    "  # minimum distance between ones\n",
    "  arange = torch.arange(len(X[0]))[None].expand(len(X), -1) \n",
    "  mask1 = (X == 1).long()\n",
    "  mask2 = (X == 2).long()\n",
    "  mask3 = (X == 3).long()\n",
    "  mask4 = (X == 4).long()\n",
    "  return (arange * mask1).amax(-1) + (arange * mask2).amax(-1) + (arange * mask3).amax(-1) + (arange * mask4).amax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80062/80062 [00:01<00:00, 57333.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64049, 16]), torch.Size([64049, 2]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/data.pt\"\n",
    "Xall, Yall = torch.load(data_path)\n",
    "# add a couple to last dimension of Xall\n",
    "Xall = torch.cat([Xall, torch.zeros(Xall.shape[0], 4, dtype=torch.long).to(Xall.device)], dim=-1)\n",
    "# Yall = making_up_target(Xall)\n",
    "BASE = 1023\n",
    "max_coeff = Yall.abs().max().item()\n",
    "# max_coeff = 50000\n",
    "Yall = basis_emb(Yall.flatten(), max_coeff, BASE) + Xall.max() + 1\n",
    "# take out sign?\n",
    "# Yall = Yall[:, 1:]\n",
    "Yall = Yall.flatten(1).clone()\n",
    "\n",
    "def loop_n_sel(X, n, accept_prob=1.):\n",
    "  return (X[:, 2*n:2*n+2] == 0).all(1) & (X[:, 2*n-1] != 0) & (torch.rand(len(X)) < accept_prob)\n",
    "\n",
    "#only take up to loop 5\n",
    "sel = loop_n_sel(Xall, 5, accept_prob=.3) | loop_n_sel(Xall, 3)\n",
    "X = Xall[sel]\n",
    "Y = Yall[sel]\n",
    "\n",
    "torch.manual_seed(10)\n",
    "perm = torch.randperm(len(X))\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "max_rotations = (X == 0).sum(1)\n",
    "n_rotations = torch.randint(max_rotations.amax(), (len(max_rotations),))\n",
    "X = X.clone()\n",
    "# for i in tqdm.trange(len(X)):\n",
    "#   rots = (n_rotations[i]%max_rotations[i]).item()\n",
    "#   X[i] = torch.roll(X[i], rots, 0).long()\n",
    "\n",
    "split = int(0.8*len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "Y_train, Y_val = Y[:split], Y[split:]\n",
    "\n",
    "sel_test = loop_n_sel(Xall, 4)\n",
    "X_test = Xall[sel_test]\n",
    "Y_test = Yall[sel_test]\n",
    "perm = torch.randperm(len(X_test))\n",
    "test_len = 50000\n",
    "X_test = X_test[perm][:test_len]\n",
    "Y_test = Y_test[perm][:test_len]\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "X_train = X_train.to(DEVICE)\n",
    "Y_train = Y_train.to(DEVICE)\n",
    "X_val = X_val.to(DEVICE)\n",
    "Y_val = Y_val.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "Y_test = Y_test.to(DEVICE)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.81M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.06e-01, val loss: 1.44e-01, ood loss: 1.94e+01, val acc: 0.89, ood acc: 0.00:   1%|          | 11/1000 [00:24<37:14,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# define the model architecture\n",
    "D_MODEL = 128\n",
    "config = GPTConfig(block_size=X.shape[1], vocab_size=Yall.max().item() + 1, n_layer=4, n_head=D_MODEL//32, n_embd=D_MODEL, output_dim=Yall.shape[-1])\n",
    "\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "model = Model(config).to(DEVICE)\n",
    "\n",
    "# define the training loop\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 256\n",
    "EVAL_FREQ = 1\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1, 1e-2, total_iters = math.ceil(len(X_train) / BATCH_SIZE) * EPOCHS)\n",
    "criterion = lambda pred, true: torch.nn.functional.cross_entropy(pred.view(-1, pred.shape[-1]), true.view(-1))\n",
    "\n",
    "\n",
    "try:\n",
    "  for epoch in (bar:=tqdm.trange(EPOCHS)):\n",
    "    if epoch % EVAL_FREQ == 0:\n",
    "      with torch.inference_mode():\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for i in range(0, len(X_val), BATCH_SIZE):\n",
    "          x_batch = X_val[i:i+BATCH_SIZE]\n",
    "          y_batch = Y_val[i:i+BATCH_SIZE]\n",
    "          y_pred = model(x_batch)\n",
    "          val_loss += criterion(y_pred, y_batch).item()\n",
    "          val_acc += (y_pred.argmax(-1) == y_batch).all(dim=1).float().mean().item()\n",
    "        val_loss /= math.ceil(len(X_val) / BATCH_SIZE)\n",
    "        val_acc /= math.ceil(len(X_val) / BATCH_SIZE)\n",
    "        \n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        for i in range(0, len(X_test), BATCH_SIZE):\n",
    "          x_batch = X_test[i:i+BATCH_SIZE]\n",
    "          y_batch = Y_test[i:i+BATCH_SIZE]\n",
    "          y_pred = model(x_batch)\n",
    "          test_loss += criterion(y_pred, y_batch).item()\n",
    "          test_acc += (y_pred.argmax(-1) == y_batch).all(dim=1).float().mean().item()\n",
    "        test_loss /= math.ceil(len(X_test) / BATCH_SIZE)\n",
    "        test_acc /= math.ceil(len(X_test) / BATCH_SIZE)\n",
    "    for i in range(0, len(X_train), BATCH_SIZE):\n",
    "      x_batch = X_train[i:i+BATCH_SIZE]\n",
    "      y_batch = Y_train[i:i+BATCH_SIZE]\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = model(x_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      # clip grad\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      bar.set_description(f\"Loss: {loss.item():.2e}, val loss: {val_loss:.2e}, ood loss: {test_loss:.2e}, val acc: {val_acc:.2f}, ood acc: {test_acc:.2f}\")\n",
    "except KeyboardInterrupt:\n",
    "  pass\n",
    "else:\n",
    "  torch.save(model, \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model):\n",
    "  with torch.no_grad():\n",
    "    loopsel = loop_n_sel(Xall, 5)\n",
    "    xloop, yloop = Xall[loopsel], Yall[loopsel]\n",
    "    print(\"number of elements: \", loopsel.sum().item())\n",
    "    perm = torch.randperm(len(xloop))\n",
    "    xloop = xloop[perm].to(DEVICE)\n",
    "    yloop = yloop[perm].to(DEVICE)\n",
    "    # yloop = yloop.sign() * yloop.abs().log()\n",
    "    # yloop = yloop.abs().log()\n",
    "    # run model on another loop\n",
    "    loss = 0\n",
    "    BS = 512\n",
    "    n_elements = 100000\n",
    "    for i in range(0, n_elements, BS):\n",
    "      x_batch = xloop[i:i+BS]\n",
    "      y_batch = yloop[i:i+BS]\n",
    "      y_pred = model(x_batch)\n",
    "      loss += torch.nn.functional.mse_loss(y_pred, y_batch, reduction=\"sum\").item()\n",
    "    loss = loss / n_elements\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.41M\n",
      "number of elements:  263880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements:  263880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0905110290527344, 5.020735427246094)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model = Model(config).to(DEVICE)\n",
    "eval_model(model), eval_model(random_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
