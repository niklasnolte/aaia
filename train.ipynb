{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import functools\n",
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "from utils import get_alibi_biases\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alibias(T,ms):\n",
    "  alibias = torch.empty(T, T)\n",
    "  row = torch.concat((torch.arange(-T+1, 0, 1), torch.arange(0, -T, -1)))\n",
    "  for i in range(T):\n",
    "    alibias[-i-1] = row[i:i+T]\n",
    "  return alibias[None,None].repeat(1, len(ms), 1,1) * ms[None,:,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an RNN\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        hidden_dim=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or d_model\n",
    "        self.fc1 = torch.nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, d_model)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.act(self.fc1(x))) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "  def __init__(self, d_model, vocab_size):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = torch.nn.Embedding(vocab_size, d_model)\n",
    "    self.layer = ResBlock(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, L = x.shape\n",
    "    x = self.embedding(x)\n",
    "    current = x[:, 0]\n",
    "    for seq_idx in range(1,L):\n",
    "      current = self.layer(current + x[:, seq_idx])\n",
    "    return current\n",
    "\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = torch.nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = torch.nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.register_buffer(\"alibias\", make_alibias(config.block_size, ms=torch.randn(config.n_head)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att += self.alibias[:, :, :T, :T]\n",
    "\n",
    "\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = torch.nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = torch.nn.GELU()\n",
    "        self.c_proj  = torch.nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wte = torch.nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            drop = torch.nn.Dropout(config.dropout),\n",
    "            h = torch.nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "        self.lm_head = torch.nn.Linear(config.n_embd, 1, bias=False)\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.lm_head(x.sum(dim=1))\n",
    "        return logits\n",
    "\n",
    "    \n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3942139, 12])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/data.pt\"\n",
    "Xall, Yall = torch.load(data_path)\n",
    "def loop_n_sel(X, n):\n",
    "  return (X[:, 2*n:2*n+2] == 0).all(1) & (X[:, 2*n-1] != 0)\n",
    "\n",
    "#only take up to loop 5\n",
    "sel = loop_n_sel(Xall, 4) | loop_n_sel(Xall, 6)\n",
    "X = Xall[sel]\n",
    "Y = Yall[sel]\n",
    "\n",
    "torch.manual_seed(10)\n",
    "perm = torch.randperm(len(X))\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "# Y = Y.sign() * Y.abs().log()\n",
    "Y = Y.abs().log()\n",
    "split = int(0.8*len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "Y_train, Y_val = Y[:split], Y[split:]\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "X_train = X_train.to(DEVICE)\n",
    "Y_train = Y_train.to(DEVICE)\n",
    "X_val = X_val.to(DEVICE)\n",
    "Y_val = Y_val.to(DEVICE)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.79M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2832:   0%|          | 0/5 [02:42<?, ?it/s]   "
     ]
    }
   ],
   "source": [
    "# define the model architecture\n",
    "D_MODEL = 128\n",
    "config = GPTConfig(block_size=12, vocab_size=len(X.unique()), n_layer=4, n_head=2, n_embd=D_MODEL)\n",
    "\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "model = Model(config).to(DEVICE)\n",
    "\n",
    "# define the training loop\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "EVAL_FREQ = 2\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1, 1e-1, total_iters = math.ceil(len(X_train) / BATCH_SIZE) * EPOCHS)\n",
    "criterion = torch.nn.MSELoss() \n",
    "\n",
    "\n",
    "try:\n",
    "  for epoch in (bar:=tqdm.trange(EPOCHS)):\n",
    "    for i in range(0, len(X_train), BATCH_SIZE):\n",
    "      x_batch = X_train[i:i+BATCH_SIZE]\n",
    "      y_batch = Y_train[i:i+BATCH_SIZE]\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = model(x_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      # clip grad\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), .1)\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "    if epoch % EVAL_FREQ == 0:\n",
    "      with torch.inference_mode():\n",
    "        loss = 0\n",
    "        for i in range(0, len(X_val), BATCH_SIZE):\n",
    "          x_batch = X_val[i:i+BATCH_SIZE]\n",
    "          y_batch = Y_val[i:i+BATCH_SIZE]\n",
    "          y_pred = model(x_batch)\n",
    "          loss += criterion(y_pred, y_batch).item()\n",
    "        loss /= len(X_val) / BATCH_SIZE\n",
    "        print(f\"Epoch {epoch}, validation loss: {loss:.4f}\")\n",
    "except KeyboardInterrupt:\n",
    "  pass\n",
    "else:\n",
    "  torch.save(model, \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model):\n",
    "  with torch.no_grad():\n",
    "    loopsel = loop_n_sel(Xall, 5)\n",
    "    xloop, yloop = Xall[loopsel], Yall[loopsel]\n",
    "    print(\"number of elements: \", loopsel.sum().item())\n",
    "    perm = torch.randperm(len(xloop))\n",
    "    xloop = xloop[perm].to(DEVICE)\n",
    "    yloop = yloop[perm].to(DEVICE)\n",
    "    # yloop = yloop.sign() * yloop.abs().log()\n",
    "    yloop = yloop.abs().log()\n",
    "    # run model on loop 6\n",
    "    loss = 0\n",
    "    BS = 512\n",
    "    n_elements = 100000\n",
    "    for i in range(0, n_elements, BS):\n",
    "      x_batch = xloop[i:i+BS]\n",
    "      y_batch = yloop[i:i+BS]\n",
    "      y_pred = model(x_batch)\n",
    "      loss += torch.nn.functional.mse_loss(y_pred, y_batch, reduction=\"sum\").item()\n",
    "    loss = loss / n_elements\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.20M\n",
      "number of elements:  263880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements:  263880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8899605859375, 9.34663110107422)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model = Model(config).to(DEVICE)\n",
    "eval_model(model), eval_model(random_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
